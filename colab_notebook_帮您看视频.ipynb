{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUX6S4fu4v1FYkDDGPJSSW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangluustb/transformers_notebook/blob/main/colab_notebook_%E5%B8%AE%E6%82%A8%E7%9C%8B%E8%A7%86%E9%A2%91.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jas--PlOmjG7"
      },
      "outputs": [],
      "source": [
        "!pip install -U openai-whisper\n",
        "!pip install paddlenlp paddlepaddle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1 transcribe\n",
        "import whisper\n",
        "model = whisper.load_model(\"small\")\n",
        "result = model.transcribe(\"test.mp4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duxWDc3Kn1wG",
        "outputId": "40bd75de-3a5e-442f-9188-fdbb43c21e70"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 461M/461M [00:04<00:00, 116MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\",\".join([x[\"text\"] for x in result[\"segments\"]])\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jS7ALhXRpjPT",
        "outputId": "caa8c383-f7b0-4a8a-b5cd-ad93d4ae0ccf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'土培的话很容易召虫,所以我家种的就比较少,主要还是以水培为主,那我们先坐下,先坐下想看空中飞人,哪来的空中飞人想看空中飞人,先坐下,这个还挺舒服的,我知道有很多小新的家里,应该也有吧,你们家里是不是也安过,就是这种调译,也安过这种调译有没有,这有很多人都有同款,我知道很多小新,也跟我是差不多的,什么,什么,说脏话了'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step2 summary\n",
        "from paddlenlp import Taskflow\n",
        "summarizer = Taskflow(\"text_summarization\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azdE-ggJqakc",
        "outputId": "5001725a-f37f-4a76-88f7-43b3cac8d2b8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[2023-02-05 03:43:41,480] [    INFO]\u001b[0m - Downloading tokenizer_config.json from https://bj.bcebos.com/paddlenlp/models/community//IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese/tokenizer_config.json\u001b[0m\n",
            "100%|██████████| 2.00/2.00 [00:00<00:00, 1.05kB/s]\n",
            "\u001b[32m[2023-02-05 03:43:43,327] [    INFO]\u001b[0m - We use pattern recognition to recognize the Tokenizer class.\u001b[0m\n",
            "\u001b[32m[2023-02-05 03:43:43,329] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.pegasus.tokenizer.PegasusChineseTokenizer'> to load 'IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese'.\u001b[0m\n",
            "\u001b[32m[2023-02-05 03:43:43,350] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/community//IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese/vocab.txt and saved to /root/.paddlenlp/models/IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese\u001b[0m\n",
            "\u001b[32m[2023-02-05 03:43:44,502] [    INFO]\u001b[0m - Downloading vocab.txt from https://bj.bcebos.com/paddlenlp/models/community//IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese/vocab.txt\u001b[0m\n",
            "100%|██████████| 365k/365k [00:05<00:00, 67.2kB/s]\n",
            "\u001b[32m[2023-02-05 03:43:51,247] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/community//IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese/added_tokens.json and saved to /root/.paddlenlp/models/IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese\u001b[0m\n",
            "\u001b[32m[2023-02-05 03:43:52,380] [    INFO]\u001b[0m - Downloading added_tokens.json from https://bj.bcebos.com/paddlenlp/models/community//IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese/added_tokens.json\u001b[0m\n",
            "100%|██████████| 2.00/2.00 [00:00<00:00, 796B/s]\n",
            "\u001b[32m[2023-02-05 03:43:53,513] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/community//IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese/special_tokens_map.json and saved to /root/.paddlenlp/models/IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese\u001b[0m\n",
            "\u001b[32m[2023-02-05 03:43:54,654] [    INFO]\u001b[0m - Downloading special_tokens_map.json from https://bj.bcebos.com/paddlenlp/models/community//IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese/special_tokens_map.json\u001b[0m\n",
            "100%|██████████| 66.0/66.0 [00:00<00:00, 47.4kB/s]\n",
            "\u001b[32m[2023-02-05 03:43:55,831] [    INFO]\u001b[0m - Already cached /root/.paddlenlp/models/IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese/tokenizer_config.json\u001b[0m\n",
            "\u001b[32m[2023-02-05 03:43:58,377] [    INFO]\u001b[0m - Standard config do not exist, loading from legacy config\u001b[0m\n",
            "\u001b[32m[2023-02-05 03:43:58,379] [    INFO]\u001b[0m - Downloading model_config.json from https://bj.bcebos.com/paddlenlp/models/community//IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese/model_config.json\u001b[0m\n",
            "100%|██████████| 732/732 [00:00<00:00, 329kB/s]\n",
            "\u001b[32m[2023-02-05 03:43:59,548] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.pegasus.modeling.PegasusForConditionalGeneration'> to load 'IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese'.\u001b[0m\n",
            "\u001b[32m[2023-02-05 03:43:59,551] [    INFO]\u001b[0m - Downloading https://bj.bcebos.com/paddlenlp/models/community//IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese/model_state.pdparams and saved to /root/.paddlenlp/models/IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese\u001b[0m\n",
            "\u001b[32m[2023-02-05 03:43:59,553] [    INFO]\u001b[0m - Downloading model_state.pdparams from https://bj.bcebos.com/paddlenlp/models/community//IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese/model_state.pdparams\u001b[0m\n",
            "100%|██████████| 1.26G/1.26G [02:27<00:00, 9.18MB/s]\n",
            "\u001b[32m[2023-02-05 03:46:28,380] [    INFO]\u001b[0m - Already cached /root/.paddlenlp/models/IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese/model_config.json\u001b[0m\n",
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.784 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.784 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['万科喊话中国房地产进入“黑铁时代”']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfXC2yKzqxaB",
        "outputId": "670f0e61-29dd-45b5-8cc3-55731e2074d0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['主题: 土培说']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "haAYyXK_r8U1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result=model.transcribe(\"InstructGPT.mp4\")"
      ],
      "metadata": {
        "id": "kIt2BhZiuC4q"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\",\".join([x[\"text\"] for x in result[\"segments\"]])\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "s5PzWphivmVJ",
        "outputId": "7e48f0f3-8f0f-4bb1-f3ef-645e5b62bb08"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'那同樣的概念呢,這邊在今年的年初 OpenAI 就推出了一個,進階版本的GPT3 叫做Intrupt GPT,那它的概念呢,基本上用了一個非常重要的概念,就是它利用了非常多的human的feedback,有點像是human in a loop,然後一直幫忙它去improve這樣子的model,那我們就來詳細的講述一下它是怎麼做的,首先呢第一步呢,第一步我們要collect一些好的demonstration,就是一些人label出來的範例,然後再用這些範例去supervise的訓練它,簡而言之我們直接從大家之前用API,Upt3的API可能又給了非常多promp,那它就直接隨便sample一個promp出來,那接著呢它就請一個human去寫,根據現在這個promp,你覺得一個好的AI模型,應該要產生什麼樣子的Output是你覺得你希望的,所以這個Output是人寫出來的,所以我們會給它很多我們期望它的Output的這種,叫做demonstration,所以這樣子一個input跟output的這個pair,它就是一個demonstration,那我們會收集非常非常多這樣子的demonstration,也就是我們希望利用這些資訊來教導我們的model,讓它知道我們希望它產生什麼樣子的結果,那這跟flam裡面的intraction tuning是有點像的,所以它的做法呢就是直接supervise的finetune這個GPTE,所以也就是直接把這個promp放進去,接著呢它是不是會繼續生成,那我們是不是就期望它生成的下一個字就是sum,然後people,所以我們就一直finetune我們的model,讓它可以看到sum的機率上升,看到people的機率上升,看到winter的機率上升,一次類推,所以原本它可能產生出來output,不是跟這句一樣的,那有可能經過了我們這邊supervisefinetune之後,產生這一個句子的機率就大增,那這是第一個step,就是supervise的finetuning,接著第二個step是我們要訓練一個rewardmodel,那這邊的rewardmodel其實就跟fl裡面的那個credit有點像,那你就把它想成是一個評分的老師就好了,那這個rewardmodel怎麼訓練呢,一樣剛我們有sampled prompt嘛,然後呢我們還有model可能不同sampled prompt機制,或不同variantmodel產生出來不同的output,假設說現在的這個gbd3,它就產生了不同幾個output,接著呢,我們要讓真人去評分,現在這些output它的分數,那這時候好,它就把它評分出來它給了一個ranking好,d大於c大於a等於b,a跟b它覺得差不多,但是d它覺得是最好的回答,那這邊的這個output都是原本model產生的,接著現在我們要做什麼事情,這個現在這個是學生嘛,學生就是一個做接龍的一個模型,我們要把評分的這個機制交給一個rewardmodel,那這個rewardmodel它就像是一個老師,它就是去評說,好現在你看到這一個promp跟一個output,我要給你幾分,rewardmodel它的model裡面的architecture,其實是不一定要跟你的gbd3一樣那麼大的,比如說你可以拿一個比較小的size的model,來做rewardmodel的訓練,不過蠻常見的一個做法是,它直接把gbd3的最後一層,就是加上一個就是一個random linear,然後讓它output一個scaler,變成是一個reward的分數,然後接著呢藉由rewardmodel,一直去訓練調整這個分數,讓它可以訓練出排名比較前面的,有比較高的reward,我們來看一下它的細節是怎麼做這個rewardmodel training的,rewardmodel training,實際上在實作的時候呢,它其實是兩個來做比較而已,所以目標呢就是去estimate這兩個input它的reward,所以使用者雖然它給了一個ranking,但實際上你把它拿來用的時候,你是把兩個output,比如說這邊有一個output是j,有一個output是k,它們的promp是一樣的,然後呢我們希望這個rewardmodel,它可以predict出,估計出,j給它Rj,k給它Rk,當成它的reward,也就是當成它這個output的分數,那我們希望如果現在使用者覺得,j比k好的話,我們希望這邊的這個reward,它們之間的差值,會變成是我們的loss,用這個方式去訓練我們的rewardmodel,訓練我們的老師怎麼去評分,因為如果說評出來是,比如說它給k,一開始隨便randon initialize,它給kRk比Rj大,這邊loss就會很大,所以它就要下降這個loss,所以漸漸的它就會把Rj,訓練的比Rk來的大,因為你使用者是preferkk大於k,所以就目標是Rj要比Rk來的高分,這就是它的loss,所以這邊的loss就很簡單,我們就是取兩個output的結果,就是yj跟yk,也就是這邊prompt,放進去之後的output,x就是我們的prompt,所以我們就在很多這樣子配,已經大家已經排好,j跟k的情況下,然後我們去optimize,把這個當成是我們的loss,所以這邊的話就是,如果大家比較preferkk,j勝過於k的話,我們就把這邊,當成是我們的loss,來去下降,那最後呢,我們會讓這個rewardmodel的值,是output一個zero mean,也就是,我們在做第三個step之前,老師評分,他平均就是給0分,然後呢,有一些人是比0分高,有一些人是附的,所以這樣子把 mean,就是設在0,因為我們比較的是他們的相對關係,所以我們是可以做一個這樣子的shift,用一個biased做shift,那第三步就是apply RL,為什麼我們要訓練剛剛那個rewardmodel呢,就是因為我們需要一個老師,幫我們打分數,這個老師其實就是我們的環境,所以現在我們就要調整我們的,GPT,做nature language的生成,然後呢,不斷的讓剛剛已經訓練好的,打分數的這個老師,給現在這個生成出來的結果打分數,所以做法就是一樣,我們先跑一個prop,然後呢我們就看現在model generate出來的output,接著呢,僅個output generate完了之後呢,我們就會丟給這個老師rewardmodel,讓他去估計現在這一組,input跟output,他可以得到幾分呢,好,然後他就會output一個reward,他的分數出來,所以他就是一個有可能是正,有可能是負的一個數值,一個scaler,就是rk,接著,這個reward就會去feedback回,我們的這個generation的model,利用 reinforcement learning的方式,去調整他的policy,也就是調整generate每一個talking的行為,大家在回想一下,第二個步驟,我們就是訓練好這個老師,所以呢老師就是會給我們評分,接著第三個步驟就是,我們要調整我們的這個generation的策略,所以每一個時間點,generate一個talking的這個action,馬上成是reinforced learning裡面的,這個action,所以整個,句子generate完了之後,我們可以得到老師的評分,所以就是最後這個episode,完整這個episode得到的reward,我們就會去調整,這整個episode上面的這個action,那PPO其實是一個policy規定的,reinforced learning的做法,那這邊不會,想數,不過你可以把它想成,他就跟其他的policy規定是很接近的,那用PPO做reinforced learning,他的式子其實就長這樣子,簡單來描述,其實就跟其他的policy規定是很像的,他就是如果用現在的這個policy去做sample,我們會希望他sample出來的,re...,他sample出來的這些結果,如果可以讓reward高的,我們就會上升做這些action的機率,那反之如果sample出來的這些episode,他的reward是低的,我們就會下降做這些action的機率,大致上是這樣子的概念,不過他這邊做了一個額外的衍生,就是PPO PTX,這個做法呢,是他多加上了後面的這一塊,就是他希望再做這個,規定的training的時候,我們除了去調整,產生每一個字generation這邊的這個規定以外,我還要讓這個規定跟原本Pretient好的這個規定是接近的,這什麼概念呢?,他的概念其實就是,因為我在調整我的這個GPT,如果我現在直接讓他用我,一個我訓練出來的老師給的分數,然後去調整,如果說我訓練這個老師他給的分數沒那麼好的話,那這樣你是不是有可能這個學生,就是很容易就學歪了,就是很容易overfit老師的這個reward分數,所以我們希望這個學生他的這個行為,要跟他Pretient的時候的那些規定不能差太多,所以這樣子一來他比較不會,因為不準確的一些reward,來大大的影響到,我們GPT上面生成的效果,好,接著剛剛的三個step就結束了,那其實第三個step,reinforced learning完了之後,我們是會在iterative回到第二個step,去重新更新我們的老師,因為你的生成是不是就變好了,所以你是不是可以去更新老師,那因為老師的那一邊,他他要做打分數嘛,所以他的那個model裡面的那個參數,因為在你也是用GPT的參數,所以他就被更新了,你在重新的調整rewardmodel的training,接著reward老師的分數,經過調整之後,你是不是第三個step又可以再重複的,進行一次reinforced learning的訓練,因為老師現在打分的結果不一樣了,我們就可以iterative的update多次,接著呢,我們就來看一下,經過這三個step,indirectGPT他的效果,首先呢,他evaluate了兩個部分,用個外部現成的data,一個呢是choose for QA,那在choose for QA,其實主要就是希望能夠,在zero shot的情況,看你有沒有辦法,正確的回答一些內容,因為他這些這些Q,可能常常是一些,大家模型可能比較容易犯錯的,一些Q,那在這個data set上面做測試的話,如果一開始的這個GPT,他的truefulness的程度大概是22%,就是大概22%是會對,那如果我們經過先做第一個階段的finetuning,就是我收集一些,人定出來的demonstration去finetune以後,其實好像沒有什麼差,可能還變差了一點點,這個truefulness,但是我們提出來的這一個,他提出來的這個indirectGPT,透過reinforced learning,然後去,上升這一個人給出來的rein,我就是人給出來的這個排名,其實可以implicit的去提升他的truefulness,因為人給出來的分數,如果你是錯,你是fake的,他就會給他比較低的排名,所以給他比較低的排名,你其實基本上你就會學到,要給他比較低的這些reward,所以實際上他是可能,reward model是某種程度上,幫助我們去評估,現在裡面的資訊,他到底有沒有錯,雖然我們並沒有明確的告訴他說,你要去看他是有錯或沒有錯,他是不是factual,但是因為人給出來的排名,可能會跟他是否真實是相關的,所以他也implicit的學到這樣子的資訊,所以利用reinforced learning,來improve我們的GPT theory之後,他的truefulness就可以提升到41%,接著另外一個是測試,他是否可能會產生一些比較不好的一些詞彙,就是如果你把他直接用在,deploy到整個某個系統上面,他產生一些toxic work可能就不好,所以我們也有一個dataset,他就是測試你產生這些toxic work的比例,所以我們會希望這個比例是越低越好的,原本的GPT是23%,那我們用一些人的demo,然後去supervised fine tune之後,他的確是可以下降這個比例的,可以降到差不多19.9,那instruct GPT其實可以再下降一些些,就是藉由這個reward的機制,因為instruct GPT就是supervised fine tuning,再之後再加上reinforced learning,那你也知道,如果我們讓他看了人寫出來的demo,他當然比較,寫出來的demo一定不會包含這些toxic work,所以他就學到了這些比較好的,這些output他的creation比較高,所以他就也比較請上去output這些,沒那麼toxic的資訊,那如果reward,為什麼會improve他的,那個hung for的這個程度呢,因為如果說,他出現一些可能有危害的這些內容,使用者labeled他也會把,現在的這個分數打得比較低,所以你這樣子用reinforced learning訓練的時候,其實你也inflict學到不要產生這些hung for的資訊,所以他雖然沒有明確的去處理這幾個目標,但是用這簡單的方法用人去給,教導這個模型,事實上是你的確可以讓他超著你希望的方向前進,不過大家其實已經在很多,練習上面都已經練習過,怎麼去利用reinforced learning調整機器的行為,不過在這邊更加證明這個行為,這個做法是非常有用的,接著呢,還做了一個非常完整的human的annotation的study,那他怎麼做呢,他就是從現在大家使用API,然後的這些promp以及產生的這些result,來讓labeled去label說,每一個面向到底,就是有沒有符合,比如說現在這個模型,他有沒有follow使用者promp裡面的這個insert,如果使用者叫他做translation,然後他最後output不是在做translation,那就是沒有辦法去follow使用者的這個insert,所以這就是對應到我們第一個希望使用,AI model對使用者是有用的,所以我們希望他可以follow我們的task,以及如果我們的promp裡面有一些constrance,那他其實應該要滿足,比如說我們叫他寫一個summary,十個字以內,然後他寫了一個二十個字的summary,那這樣子其實也不對嘛,並沒有follow我們的constrance,對 那就對我們來講,他就不是一個有用的AI,所以這兩個面向,我們就請human去label,他到底有沒有follow我們的instruction,有沒有滿足我們的constrance,那這都是對應評估,這一個model他的usefulness,接著我們去看一樣事情,人去label說在這個output裡面,有沒有一些hostnation產生,那這就是對應有沒有honest,接著剛剛我們有提到humpful,這件事情是非常難具體的評估,因為根據使用的情境,我們其實很難確定什麼叫做humpful,所以在這裡他就請human,標記了不同面向的明確的aspect,比如說你現在使用的,你現在產生的output,是不是對於一個consumer的assistant,其實是沒那麼proper的,那因為如果我們是要使用成,變成是一個product,我們會希望對於使用者體驗是好的,所以你使用的詞彙,你也不能很粗魯啊,不是像你跟朋友這樣聊天一樣,我們希望他是比較有禮貌的,然後資訊給的是比較完整完善的,所以我們就請人去標說,這樣子的內容,這樣子的口氣口吻等等,是不是以一個customer的sistence,是appropriate,接著還有裡面有沒有包含一些sexual的content啊,volet content啊,以及在這裡面有沒有就是鼓勵,一些不好的行為,比如說恐怖攻擊啊或自殺等等這些不好的行為,如果你產生這些內容,其實就很危險嘛,然後接著呢就是欺負,欺負別人啊、敵會別人啊,然後給一些不好的建議,比如說你還不如就,從樓上跳下去吧,這樣子就是一個非常不好的建議,所以就算他不一定會直接,take你的建議,你還是會有他的危險在,所以我們就請大家去標說,每一個面向他到底是不是存在,那後面這兩個就比較,與前面的這三個主要的objective沒那麼相關,就是這邊的這個gbt,他是不是表達自己的意見啊等等,接著over all,請human給他一個一到七分over all的這個quality,現在這個回答他的quality覺得如何,所以呢我們就來看一下這邊他的,reaction的結果,這邊ppo跟ppo ptx就是,剛剛提到最後經過reinforced learning的兩個model,那加上ptx的就是,他有把pre-cham那邊的ditribution考慮進來,那supervised phy tuning就是只做了第一個step的結果,那最左邊的是直接使用的gbt,那中間的這個藍色的gbt跨乎pump it,就是我們利用了一些,有點像是給他比較多的instruction,讓他比較能夠follow我們的,task做事,那也比較容易控制蓋著他產生比較好的結果,所以大致上可以把它想成是,一個比較好的gbt使用的狀況,那在這邊可以看到第一個面向就是,在剛剛看到,這些使用API的這些內容上面,他有沒有follow正確的指令,那如果是,做了supervised phy tuning,其實你會比原本的gbt更follow指令,尤其是很好的使用gbt更follow指令,但是你加了reinforced learning,可以再更進一步的improve,讓他更好的follow指令,也就是說因為reinforced learning,其實你比較follow指令的那些,基本上人就是會給拍著比較高分,比較前面,所以你經由reinforced learning,這樣子的機制,你也可以鼓勵他去follow指令,那這邊的話就是,你有沒有follow裡面的concern,一樣他的performance的improve也是非常的顯著,那這邊的話是hospination,這邊的情況就有點不太一樣,他不是這樣子遞解的,你可以看到,如果你好好的使用hospination是,可以比直接使用gbt來的低一點點,所以這算是我們的比較base line,那如果你讓人的demo去做supervised phy tuning,其實你是可以最大的下這樣hospination的,因為畢竟他就是supervised,在你這樣的data上面學了,不過這樣子的diverty也會相較之下,比較低一點點,那有的時候我們有說,有一些hospination他其實,有可能是好的,他其實是補充你更好,更多的資訊,所以有的時候,你其實也會希望他比較diverse一點,更informative一點,所以加上這reinforcement learning,畢竟你的reinforcement learning的這個理由,相較於你直接給他一個demo來講,他是比較鬆的,所以經過reinforcement learning之後呢,他的hospination是會提升一些些,不過跟你好好的使用gbt比,其實還是有下這樣的,那最後就是你使用的這些language,是不是在一個customer的assistance,情境是ok的,其實原本的gbt就做得還蠻不錯的,那他可以再上升一些些,可以更鼓勵他去說,更禮貌的一些內容,跟口氣行為會變得更適合,那最後呢,他比較了這些over old performance,以及另外兩個,其他group提出來的follow instruction的,這些model,就是blend就是我們前面有講過,那p0的話呢是一個類似,也是follow instruction的一個model,那你可以看到說,在human的evasion上面,他覺得PPO這樣子的做法,經過reinforcement learning,他是顯著的比其他來的都好,那他也說,如果你有好好的使用gbt的話,其實你是可以跟有做,instruction tuning,可以得到差不多的效果,不過這樣子就,取決於你要好好的使用他啦,當然如果你直接用,他可能就不一定會跟你,好好地給他這個prompt,跟指令來得好,我們可以看一下他的結果,那這邊是一個,因為他是instruct gbt,他是直接improve在有訓練到code的,那一個model,pre-tune model上面,所以呢他是也可以做生成code,跟理解code的這件事情,所以你可以問他說,喔這個在這個程式裡面,see the list是幹嘛用的,那原本的產生的這些結果,可能就是反正他就是儲存,see the value,感覺是一個,也不能說他不對,但是就覺得有點廢話的一個內容,好那這邊的這個instruct gbt,你看他回答是不是就更好,他是儲存這個東西,但是你要告訴我說這個東西是什麼,這就是儲存這個,binomial coefficient的這個,function他的那個value,所以實際上這才是,使用者想要問的東西嘛,所以實際上你的這個內容,才真的有幫助到使用者,而且他回答也才是對的,當然這是一個true pick的結果,不過我覺得非常的不錯'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 繁体中文转简体\n",
        "text_s=cht_2_chs(text)"
      ],
      "metadata": {
        "id": "mZcwsBNcxXqg"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "OmrfsKOmt4EO",
        "outputId": "26a4601d-c5b2-4036-b564-cb0e117f1d42"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'那同样的概念呢,这边在今年的年初 OpenAI 就推出了一个,进阶版本的GPT3 叫做Intrupt GPT,那它的概念呢,基本上用了一个非常重要的概念,就是它利用了非常多的human的feedback,有点像是human in a loop,然后一直帮忙它去improve这样子的model,那我们就来详细的讲述一下它是怎么做的,首先呢第一步呢,第一步我们要collect一些好的demonstration,就是一些人label出来的范例,然后再用这些范例去supervise的训练它,简而言之我们直接从大家之前用API,Upt3的API可能又给了非常多promp,那它就直接随便sample一个promp出来,那接著呢它就请一个human去写,根据现在这个promp,你觉得一个好的AI模型,应该要产生什么样子的Output是你觉得你希望的,所以这个Output是人写出来的,所以我们会给它很多我们期望它的Output的这种,叫做demonstration,所以这样子一个input跟output的这个pair,它就是一个demonstration,那我们会收集非常非常多这样子的demonstration,也就是我们希望利用这些资讯来教导我们的model,让它知道我们希望它产生什么样子的结果,那这跟flam里面的intraction tuning是有点像的,所以它的做法呢就是直接supervise的finetune这个GPTE,所以也就是直接把这个promp放进去,接著呢它是不是会继续生成,那我们是不是就期望它生成的下一个字就是sum,然后people,所以我们就一直finetune我们的model,让它可以看到sum的机率上升,看到people的机率上升,看到winter的机率上升,一次类推,所以原本它可能产生出来output,不是跟这句一样的,那有可能经过了我们这边supervisefinetune之后,产生这一个句子的机率就大增,那这是第一个step,就是supervise的finetuning,接著第二个step是我们要训练一个rewardmodel,那这边的rewardmodel其实就跟fl里面的那个credit有点像,那你就把它想成是一个评分的老师就好了,那这个rewardmodel怎么训练呢,一样刚我们有sampled prompt嘛,然后呢我们还有model可能不同sampled prompt机制,或不同variantmodel产生出来不同的output,假设说现在的这个gbd3,它就产生了不同几个output,接著呢,我们要让真人去评分,现在这些output它的分数,那这时候好,它就把它评分出来它给了一个ranking好,d大于c大于a等于b,a跟b它觉得差不多,但是d它觉得是最好的回答,那这边的这个output都是原本model产生的,接著现在我们要做什么事情,这个现在这个是学生嘛,学生就是一个做接龙的一个模型,我们要把评分的这个机制交给一个rewardmodel,那这个rewardmodel它就像是一个老师,它就是去评说,好现在你看到这一个promp跟一个output,我要给你几分,rewardmodel它的model里面的architecture,其实是不一定要跟你的gbd3一样那么大的,比如说你可以拿一个比较小的size的model,来做rewardmodel的训练,不过蛮常见的一个做法是,它直接把gbd3的最后一层,就是加上一个就是一个random linear,然后让它output一个scaler,变成是一个reward的分数,然后接著呢借由rewardmodel,一直去训练调整这个分数,让它可以训练出排名比较前面的,有比较高的reward,我们来看一下它的细节是怎么做这个rewardmodel training的,rewardmodel training,实际上在实作的时候呢,它其实是两个来做比较而已,所以目标呢就是去estimate这两个input它的reward,所以使用者虽然它给了一个ranking,但实际上你把它拿来用的时候,你是把两个output,比如说这边有一个output是j,有一个output是k,它们的promp是一样的,然后呢我们希望这个rewardmodel,它可以predict出,估计出,j给它Rj,k给它Rk,当成它的reward,也就是当成它这个output的分数,那我们希望如果现在使用者觉得,j比k好的话,我们希望这边的这个reward,它们之间的差值,会变成是我们的loss,用这个方式去训练我们的rewardmodel,训练我们的老师怎么去评分,因为如果说评出来是,比如说它给k,一开始随便randon initialize,它给kRk比Rj大,这边loss就会很大,所以它就要下降这个loss,所以渐渐的它就会把Rj,训练的比Rk来的大,因为你使用者是preferkk大于k,所以就目标是Rj要比Rk来的高分,这就是它的loss,所以这边的loss就很简单,我们就是取两个output的结果,就是yj跟yk,也就是这边prompt,放进去之后的output,x就是我们的prompt,所以我们就在很多这样子配,已经大家已经排好,j跟k的情况下,然后我们去optimize,把这个当成是我们的loss,所以这边的话就是,如果大家比较preferkk,j胜过于k的话,我们就把这边,当成是我们的loss,来去下降,那最后呢,我们会让这个rewardmodel的值,是output一个zero mean,也就是,我们在做第三个step之前,老师评分,他平均就是给0分,然后呢,有一些人是比0分高,有一些人是附的,所以这样子把 mean,就是设在0,因为我们比较的是他们的相对关系,所以我们是可以做一个这样子的shift,用一个biased做shift,那第三步就是apply RL,为什么我们要训练刚刚那个rewardmodel呢,就是因为我们需要一个老师,帮我们打分数,这个老师其实就是我们的环境,所以现在我们就要调整我们的,GPT,做nature language的生成,然后呢,不断的让刚刚已经训练好的,打分数的这个老师,给现在这个生成出来的结果打分数,所以做法就是一样,我们先跑一个prop,然后呢我们就看现在model generate出来的output,接著呢,仅个output generate完了之后呢,我们就会丢给这个老师rewardmodel,让他去估计现在这一组,input跟output,他可以得到几分呢,好,然后他就会output一个reward,他的分数出来,所以他就是一个有可能是正,有可能是负的一个数值,一个scaler,就是rk,接著,这个reward就会去feedback回,我们的这个generation的model,利用 reinforcement learning的方式,去调整他的policy,也就是调整generate每一个talking的行为,大家在回想一下,第二个步骤,我们就是训练好这个老师,所以呢老师就是会给我们评分,接著第三个步骤就是,我们要调整我们的这个generation的策略,所以每一个时间点,generate一个talking的这个action,马上成是reinforced learning里面的,这个action,所以整个,句子generate完了之后,我们可以得到老师的评分,所以就是最后这个episode,完整这个episode得到的reward,我们就会去调整,这整个episode上面的这个action,那PPO其实是一个policy规定的,reinforced learning的做法,那这边不会,想数,不过你可以把它想成,他就跟其他的policy规定是很接近的,那用PPO做reinforced learning,他的式子其实就长这样子,简单来描述,其实就跟其他的policy规定是很像的,他就是如果用现在的这个policy去做sample,我们会希望他sample出来的,re...,他sample出来的这些结果,如果可以让reward高的,我们就会上升做这些action的机率,那反之如果sample出来的这些episode,他的reward是低的,我们就会下降做这些action的机率,大致上是这样子的概念,不过他这边做了一个额外的衍生,就是PPO PTX,这个做法呢,是他多加上了后面的这一块,就是他希望再做这个,规定的training的时候,我们除了去调整,产生每一个字generation这边的这个规定以外,我还要让这个规定跟原本Pretient好的这个规定是接近的,这什么概念呢?,他的概念其实就是,因为我在调整我的这个GPT,如果我现在直接让他用我,一个我训练出来的老师给的分数,然后去调整,如果说我训练这个老师他给的分数没那么好的话,那这样你是不是有可能这个学生,就是很容易就学歪了,就是很容易overfit老师的这个reward分数,所以我们希望这个学生他的这个行为,要跟他Pretient的时候的那些规定不能差太多,所以这样子一来他比较不会,因为不准确的一些reward,来大大的影响到,我们GPT上面生成的效果,好,接著刚刚的三个step就结束了,那其实第三个step,reinforced learning完了之后,我们是会在iterative回到第二个step,去重新更新我们的老师,因为你的生成是不是就变好了,所以你是不是可以去更新老师,那因为老师的那一边,他他要做打分数嘛,所以他的那个model里面的那个参数,因为在你也是用GPT的参数,所以他就被更新了,你在重新的调整rewardmodel的training,接著reward老师的分数,经过调整之后,你是不是第三个step又可以再重复的,进行一次reinforced learning的训练,因为老师现在打分的结果不一样了,我们就可以iterative的update多次,接著呢,我们就来看一下,经过这三个step,indirectGPT他的效果,首先呢,他evaluate了两个部分,用个外部现成的data,一个呢是choose for QA,那在choose for QA,其实主要就是希望能够,在zero shot的情况,看你有没有办法,正确的回答一些内容,因为他这些这些Q,可能常常是一些,大家模型可能比较容易犯错的,一些Q,那在这个data set上面做测试的话,如果一开始的这个GPT,他的truefulness的程度大概是22%,就是大概22%是会对,那如果我们经过先做第一个阶段的finetuning,就是我收集一些,人定出来的demonstration去finetune以后,其实好像没有什么差,可能还变差了一点点,这个truefulness,但是我们提出来的这一个,他提出来的这个indirectGPT,透过reinforced learning,然后去,上升这一个人给出来的rein,我就是人给出来的这个排名,其实可以implicit的去提升他的truefulness,因为人给出来的分数,如果你是错,你是fake的,他就会给他比较低的排名,所以给他比较低的排名,你其实基本上你就会学到,要给他比较低的这些reward,所以实际上他是可能,reward model是某种程度上,帮助我们去评估,现在里面的资讯,他到底有没有错,虽然我们并没有明确的告诉他说,你要去看他是有错或没有错,他是不是factual,但是因为人给出来的排名,可能会跟他是否真实是相关的,所以他也implicit的学到这样子的资讯,所以利用reinforced learning,来improve我们的GPT theory之后,他的truefulness就可以提升到41%,接著另外一个是测试,他是否可能会产生一些比较不好的一些词汇,就是如果你把他直接用在,deploy到整个某个系统上面,他产生一些toxic work可能就不好,所以我们也有一个dataset,他就是测试你产生这些toxic work的比例,所以我们会希望这个比例是越低越好的,原本的GPT是23%,那我们用一些人的demo,然后去supervised fine tune之后,他的确是可以下降这个比例的,可以降到差不多19.9,那instruct GPT其实可以再下降一些些,就是借由这个reward的机制,因为instruct GPT就是supervised fine tuning,再之后再加上reinforced learning,那你也知道,如果我们让他看了人写出来的demo,他当然比较,写出来的demo一定不会包含这些toxic work,所以他就学到了这些比较好的,这些output他的creation比较高,所以他就也比较请上去output这些,没那么toxic的资讯,那如果reward,为什么会improve他的,那个hung for的这个程度呢,因为如果说,他出现一些可能有危害的这些内容,使用者labeled他也会把,现在的这个分数打得比较低,所以你这样子用reinforced learning训练的时候,其实你也inflict学到不要产生这些hung for的资讯,所以他虽然没有明确的去处理这几个目标,但是用这简单的方法用人去给,教导这个模型,事实上是你的确可以让他超著你希望的方向前进,不过大家其实已经在很多,练习上面都已经练习过,怎么去利用reinforced learning调整机器的行为,不过在这边更加证明这个行为,这个做法是非常有用的,接著呢,还做了一个非常完整的human的annotation的study,那他怎么做呢,他就是从现在大家使用API,然后的这些promp以及产生的这些result,来让labeled去label说,每一个面向到底,就是有没有符合,比如说现在这个模型,他有没有follow使用者promp里面的这个insert,如果使用者叫他做translation,然后他最后output不是在做translation,那就是没有办法去follow使用者的这个insert,所以这就是对应到我们第一个希望使用,AI model对使用者是有用的,所以我们希望他可以follow我们的task,以及如果我们的promp里面有一些constrance,那他其实应该要满足,比如说我们叫他写一个summary,十个字以内,然后他写了一个二十个字的summary,那这样子其实也不对嘛,并没有follow我们的constrance,对 那就对我们来讲,他就不是一个有用的AI,所以这两个面向,我们就请human去label,他到底有没有follow我们的instruction,有没有满足我们的constrance,那这都是对应评估,这一个model他的usefulness,接著我们去看一样事情,人去label说在这个output里面,有没有一些hostnation产生,那这就是对应有没有honest,接著刚刚我们有提到humpful,这件事情是非常难具体的评估,因为根据使用的情境,我们其实很难确定什么叫做humpful,所以在这里他就请human,标记了不同面向的明确的aspect,比如说你现在使用的,你现在产生的output,是不是对于一个consumer的assistant,其实是没那么proper的,那因为如果我们是要使用成,变成是一个product,我们会希望对于使用者体验是好的,所以你使用的词汇,你也不能很粗鲁啊,不是像你跟朋友这样聊天一样,我们希望他是比较有礼貌的,然后资讯给的是比较完整完善的,所以我们就请人去标说,这样子的内容,这样子的口气口吻等等,是不是以一个customer的sistence,是appropriate,接著还有里面有没有包含一些sexual的content啊,volet content啊,以及在这里面有没有就是鼓励,一些不好的行为,比如说恐怖攻击啊或自杀等等这些不好的行为,如果你产生这些内容,其实就很危险嘛,然后接著呢就是欺负,欺负别人啊、敌会别人啊,然后给一些不好的建议,比如说你还不如就,从楼上跳下去吧,这样子就是一个非常不好的建议,所以就算他不一定会直接,take你的建议,你还是会有他的危险在,所以我们就请大家去标说,每一个面向他到底是不是存在,那后面这两个就比较,与前面的这三个主要的objective没那么相关,就是这边的这个gbt,他是不是表达自己的意见啊等等,接著over all,请human给他一个一到七分over all的这个quality,现在这个回答他的quality觉得如何,所以呢我们就来看一下这边他的,reaction的结果,这边ppo跟ppo ptx就是,刚刚提到最后经过reinforced learning的两个model,那加上ptx的就是,他有把pre-cham那边的ditribution考虑进来,那supervised phy tuning就是只做了第一个step的结果,那最左边的是直接使用的gbt,那中间的这个蓝色的gbt跨乎pump it,就是我们利用了一些,有点像是给他比较多的instruction,让他比较能够follow我们的,task做事,那也比较容易控制盖著他产生比较好的结果,所以大致上可以把它想成是,一个比较好的gbt使用的状况,那在这边可以看到第一个面向就是,在刚刚看到,这些使用API的这些内容上面,他有没有follow正确的指令,那如果是,做了supervised phy tuning,其实你会比原本的gbt更follow指令,尤其是很好的使用gbt更follow指令,但是你加了reinforced learning,可以再更进一步的improve,让他更好的follow指令,也就是说因为reinforced learning,其实你比较follow指令的那些,基本上人就是会给拍著比较高分,比较前面,所以你经由reinforced learning,这样子的机制,你也可以鼓励他去follow指令,那这边的话就是,你有没有follow里面的concern,一样他的performance的improve也是非常的显著,那这边的话是hospination,这边的情况就有点不太一样,他不是这样子递解的,你可以看到,如果你好好的使用hospination是,可以比直接使用gbt来的低一点点,所以这算是我们的比较base line,那如果你让人的demo去做supervised phy tuning,其实你是可以最大的下这样hospination的,因为毕竟他就是supervised,在你这样的data上面学了,不过这样子的diverty也会相较之下,比较低一点点,那有的时候我们有说,有一些hospination他其实,有可能是好的,他其实是补充你更好,更多的资讯,所以有的时候,你其实也会希望他比较diverse一点,更informative一点,所以加上这reinforcement learning,毕竟你的reinforcement learning的这个理由,相较于你直接给他一个demo来讲,他是比较松的,所以经过reinforcement learning之后呢,他的hospination是会提升一些些,不过跟你好好的使用gbt比,其实还是有下这样的,那最后就是你使用的这些language,是不是在一个customer的assistance,情境是ok的,其实原本的gbt就做得还蛮不错的,那他可以再上升一些些,可以更鼓励他去说,更礼貌的一些内容,跟口气行为会变得更适合,那最后呢,他比较了这些over old performance,以及另外两个,其他group提出来的follow instruction的,这些model,就是blend就是我们前面有讲过,那p0的话呢是一个类似,也是follow instruction的一个model,那你可以看到说,在human的evasion上面,他觉得PPO这样子的做法,经过reinforcement learning,他是显著的比其他来的都好,那他也说,如果你有好好的使用gbt的话,其实你是可以跟有做,instruction tuning,可以得到差不多的效果,不过这样子就,取决于你要好好的使用他啦,当然如果你直接用,他可能就不一定会跟你,好好地给他这个prompt,跟指令来得好,我们可以看一下他的结果,那这边是一个,因为他是instruct gbt,他是直接improve在有训练到code的,那一个model,pre-tune model上面,所以呢他是也可以做生成code,跟理解code的这件事情,所以你可以问他说,喔这个在这个程式里面,see the list是干嘛用的,那原本的产生的这些结果,可能就是反正他就是储存,see the value,感觉是一个,也不能说他不对,但是就觉得有点废话的一个内容,好那这边的这个instruct gbt,你看他回答是不是就更好,他是储存这个东西,但是你要告诉我说这个东西是什么,这就是储存这个,binomial coefficient的这个,function他的那个value,所以实际上这才是,使用者想要问的东西嘛,所以实际上你的这个内容,才真的有帮助到使用者,而且他回答也才是对的,当然这是一个true pick的结果,不过我觉得非常的不错'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer(text_s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i6k1zanrqhf",
        "outputId": "3bd525fb-82f6-4a48-935f-5e4d60a9e9b8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['openai 推出了一个gpt3的概念']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 总结的效果还是不太好，应该更精细一些（分段、每段总结）"
      ],
      "metadata": {
        "id": "xMiOe9L6xlk9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}